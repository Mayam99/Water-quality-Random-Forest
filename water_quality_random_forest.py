# -*- coding: utf-8 -*-
"""Water-quality-Random-Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_UIBM9lPB6JipJXvD66TZqiYxqs33pLV

#Introduction
##Water Quality Dataset Overview
###Water is essential for life, and its quality is crucial for health, ecosystems, and the economy. The Water Quality Dataset from Kaggle provides valuable data for analyzing and classifying the quality of water. This dataset is instrumental for researchers, data scientists, and environmentalists in developing models to predict and improve water quality.

##Dataset Description
###The dataset consists of various chemical parameters tested in water samples to classify the quality of the water. These parameters include pH level, hardness, solids, chloramines, sulfate, conductivity, organic carbon, trihalomethanes, and turbidity. Each parameter provides insights into different aspects of water quality, and their collective analysis helps in determining whether the water is safe for consumption and other uses.

##Content
####What's inside is more than just rows and columns. You can see water ingredients listed as column names.

##Description
###All attributes are numeric variables and they are listed bellow:

* aluminium - dangerous if greater than 2.8
* ammonia - dangerous if greater than 32.5
* arsenic - dangerous if greater than 0.01
* barium - dangerous if greater than 2
* cadmium - dangerous if greater than 0.005
* chloramine - dangerous if greater than 4
* chromium - dangerous if greater than 0.1
* copper - dangerous if greater than 1.3
* flouride - dangerous if greater than 1.5
* bacteria - dangerous if greater than 0
* viruses - dangerous if greater than 0
* lead - dangerous if greater than 0.015
* nitrates - dangerous if greater than 10
* nitrites - dangerous if greater than 1
* mercury - dangerous if greater than 0.002
* perchlorate - dangerous if greater than 56
* radium - dangerous if greater than 5
* selenium - dangerous if greater than 0.5
* silver - dangerous if greater than 0.1
* uranium - dangerous if greater than 0.3

###Target Variable:
* is_safe - class attribute {0 - not safe, 1 - safe}

##Objective
###The main objective of this analysis is to classify water samples as potable or non-potable based on the given chemical properties. By employing machine learning algorithms, we aim to build a predictive model that can accurately determine water quality, aiding in the management and treatment of water resources.

##Applications
* Public Health: Ensuring safe drinking water and preventing waterborne diseases.
* Environmental Monitoring: Assessing the impact of pollution on water bodies.
* Water Treatment: Optimizing water purification processes.
* Policy Making: Informing regulations and standards for water quality.

##Conclusion
###By understanding and analyzing the Water Quality Dataset, we can make significant strides in improving water safety and quality. This notebook will guide you through the steps of preprocessing the data, exploring machine learning model, and evaluating their performance in classifying water quality.



##Source
https://www.kaggle.com/datasets/mssmartypants/water-quality/data
"""

import pandas as pd #Pandas is a powerful library for data manipulation and analysis.
import numpy as np #NumPy is a powerful tool for numerical computations in Python.

df= pd.read_csv("Water Quality.csv") #The df = pd.read_csv line reads a CSV file into a DataFrame named df using the pandas library.

df.head() #Displays the first few rows of the dataset

"""##Exploratory data analysis (EDA)

"""

df.shape #Displays the total count of the Rows and Columns respectively.

df.columns #Displays the names of the columns

df.info() #Displays the total count of values present in the particular column along with the null count and data type.

df.isnull().sum() #Displays the total count of the null values in the particular columns.

"""###As we can check there is no missing or null value in the dataset."""

# Check for non-numeric values in numeric columns
non_numeric = df.apply(lambda x: pd.to_numeric(x, errors='coerce')).isna()
print(df[non_numeric.any(axis=1)])

"""1) df.apply(lambda x: pd.to_numeric(x, errors='coerce')):

df.apply(...) applies a function along the specified axis (default is columns).
lambda x: pd.to_numeric(x, errors='coerce') is the function being applied to each column x in the DataFrame.
pd.to_numeric(x, errors='coerce') attempts to convert each element in the column x to a numeric type. If it encounters a non-numeric value (e.g., '#NUM!'), it coerces that value to NaN (Not a Number).


2) isna():

This method returns a DataFrame of the same shape as df, with boolean values (True or False). Each element is True if the corresponding element in df is NaN, and False otherwise.

3) non_numeric.any(axis=1):

non_numeric is a DataFrame where each element is True if the corresponding element in the original DataFrame df could not be converted to a numeric type and was coerced to NaN.

.any(axis=1) checks each row to see if any of the values in that row are True. It returns a Series of boolean values where True indicates that the row contains at least one NaN.

4) df[non_numeric.any(axis=1)]:

This filters the original DataFrame df to return only those rows that contain at least one non-numeric value that was coerced to NaN.

###Why This Code is Needed


* Identify Problematic Rows: The code helps identify rows in the dataset that contain non-numeric values in columns expected to be numeric. This is crucial for data cleaning and preprocessing, as machine learning models require numeric input.

* Ensure Data Integrity: By identifying these rows, we can take appropriate actions (e.g., removing or imputing these values) to ensure the integrity of the dataset.

* Prevent Errors: It helps prevent errors during the conversion of data types and during the training of machine learning models, which expect numeric input.

"""

# Drop rows with non-numeric values
df_cleaned = df.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()

print(df_cleaned.head())

"""###The code converts all columns to numeric, coercing non-numeric values to NaN, then drops any rows containing NaN values, effectively removing rows with non-numeric entries."""

# Replace non-numeric values with the mean of the column
df_cleaned = df.apply(lambda x: pd.to_numeric(x, errors='coerce'))
df_cleaned.fillna(df_cleaned.mean(), inplace=True)

df_cleaned.head()

"""###The code converts all columns to numeric, coercing non-numeric values to NaN, and then replaces these NaN values with the mean of their respective columns."""

# Ensure all columns are converted to numeric types
df_cleaned = df_cleaned.astype(float)

print(df_cleaned.dtypes)
#The code converts all columns in the DataFrame to the float data type and prints the data types of the columns to confirm the conversion.

x  = df_cleaned.iloc[ : , :-1].values #This line extracts all the columns except the last one and stores them in the variable x.
print(x) #This line prints the contents of the x array, which contains all the feature values from the DataFrame.
print('-'*40) #This line prints a separator line consisting of 40 hyphens. This is used to visually separate the printed output of the features from the target values.
y = df.iloc[ : , -1].values #This line extracts the last column (assumed to be the target variable) and stores it in the variable y.
print(y) #This line prints the contents of the y array, which contains the target values from the DataFrame.

import matplotlib.pyplot as plt
import seaborn as sns
# Optional: Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df_cleaned.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

from sklearn.preprocessing import StandardScaler #This line imports the StandardScaler class from the sklearn.preprocessing module. StandardScaler is used to standardize features by removing the mean and scaling to unit variance.


scaling      = StandardScaler() #This line creates an instance of the StandardScaler class, which will be used to fit and transform the feature data.
scaled_input = scaling.fit_transform(x) #The fit_transform method combines both steps: it first fits the StandardScaler to the data and then transforms the data. The resulting scaled_input array contains the standardized features

scaled_input #This line outputs the scaled feature values.

"""##Purpose and Usage
###The purpose of this code is to standardize the features in the dataset. Standardization is an important preprocessing step in many machine learning algorithms for the following reasons:

###1) Normalization:
####Standardization ensures that each feature has a mean of 0 and a standard deviation of 1. This is important for algorithms that are sensitive to the scale of the input features, such as gradient descent-based algorithms.

###2) Improved Convergence:
####For algorithms like gradient descent, having standardized features can lead to faster and more stable convergence. This is because the optimization process benefits from features that are on a similar scale.

###3) Handling Different Units:
####When features have different units or scales (e.g., age in years, income in dollars), standardization ensures that no single feature dominates the others due to its scale.

###4) Consistency:
####Standardization provides a consistent way to preprocess data, making the features more comparable and ensuring that the model's performance is not influenced by varying scales of input features.
"""

from sklearn.model_selection import train_test_split #This line imports the train_test_split function from the sklearn.model_selection module. This function is used to split arrays or matrices into random train and test subsets.

x_train, x_test, y_train, y_test = train_test_split(scaled_input, y, test_size=0.2, random_state=42)

"""###This line splits the scaled_input and y arrays into training and testing sets. Here's a breakdown of the parameters used:
###scaled_input: The input features to be split.
###y: The target variable to be split.
###test_size=0.2: Specifies that 20% of the data should be used for the test set and 80% for the training set.
###random_state=42: A random seed to ensure reproducibility. Using the same seed will produce the same split every time.
"""

x.shape, y.shape #The x.shape and y.shape attributes return the dimensions of the x (features) and y (target) arrays, respectively, indicating the number of samples and features (for x) or target values (for y).

x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""###The x_train.shape, x_test.shape, y_train.shape, and y_test.shape attributes return the dimensions of the training and test sets for features and target variables, showing the number of samples and features (for x_train and x_test) or target values (for y_train and y_test)."""

from sklearn.ensemble import RandomForestClassifier #The code from sklearn.ensemble import RandomForestClassifier imports the RandomForestClassifier class from scikit-learn,

Random_Forest_Model = RandomForestClassifier() #Random_Forest_Model = RandomForestClassifier() creates an instance of the RandomForestClassifier model.

Random_Forest_Model.fit(x_train,y_train) #Random_Forest_Model.fit(x_train, y_train) trains the RandomForestClassifier model using the training features (x_train) and target values (y_train).

y_pred = Random_Forest_Model.predict(x_test) #uses the trained RandomForestClassifier model to make predictions on the test features (x_test)
y_pred #y_pred stores these predicted target values.

from sklearn.metrics import accuracy_score #Imports the function to calculate accuracy

print("Accuracy_Score:",np.round(accuracy_score(y_test, y_pred),2)*100,"%")
# Computes the accuracy score, rounds it to two decimal places, converts it to a percentage, and prints it

y_test
#y_test contains the true target values for the test set, used to evaluate the performance of the model's predictions.

x_test[2]#.reshape(1,11)
#x_test[2].reshape(1, 11) selects the third test sample and reshapes it into a 2D array with one row and 11 columns for making a single prediction.

x_test[2].shape
#x_test[2].shape returns the dimensions of the third test sample, showing it as a 1D array with 20 features.

x_test[2].reshape(1,20)
#x_test[2].reshape(1, 20) reshapes the third test sample from a 1D array with 11 features into a 2D array with 1 row and 11 columns for model prediction

x_test[2].reshape(1,20).shape
#x_test[2].reshape(1, 20).shape returns the shape of the reshaped third test sample as (1, 11), indicating 1 sample with 20 features.

print("ACTUAL OUTPUT    : ",y_test[2])
print("PREDICTED OUTPUT : ",Random_Forest_Model.predict(x_test[2].reshape(1,20))[0])